<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="generator" content="VuePress 2.0.0-rc.18">
    <script>
      (function() {
        const userMode = localStorage.getItem('vuepress-reco-color-scheme') || 'auto';
        const systemDarkMode = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;

        if (userMode === 'dark' || (userMode === 'auto' && systemDarkMode)) {
          document.documentElement.classList.toggle('dark', true);
        }
      })();
    </script>
    <title>CUDA 分享 | AQIU的日志</title><meta name="description" content="随便写写...">
    <link rel="preload" href="/assets/style-C2pV2qh2.css" as="style"><link rel="stylesheet" href="/assets/style-C2pV2qh2.css">
    <link rel="modulepreload" href="/assets/app-vAERMEX6.js"><link rel="modulepreload" href="/assets/4.html-DJ9EFYkE.js">
    <link rel="prefetch" href="/assets/timeline.html-BJpoaFI2.js" as="script"><link rel="prefetch" href="/assets/posts.html-BuqMDWee.js" as="script"><link rel="prefetch" href="/assets/friendship-link.html-DiYcbTb0.js" as="script"><link rel="prefetch" href="/assets/1.html-DZQcgPA_.js" as="script"><link rel="prefetch" href="/assets/1.html-727lEK8s.js" as="script"><link rel="prefetch" href="/assets/1.html-B4CMUDyF.js" as="script"><link rel="prefetch" href="/assets/1.html-xaGiOriv.js" as="script"><link rel="prefetch" href="/assets/1.html-CUCVfy4l.js" as="script"><link rel="prefetch" href="/assets/1.html-CNxXOS-o.js" as="script"><link rel="prefetch" href="/assets/1.html-Bqq7oIoL.js" as="script"><link rel="prefetch" href="/assets/1.html-BB6dfRMy.js" as="script"><link rel="prefetch" href="/assets/1.html-b2-IMmiE.js" as="script"><link rel="prefetch" href="/assets/1.html-C9uHEh-J.js" as="script"><link rel="prefetch" href="/assets/1.html-CbS4ee2X.js" as="script"><link rel="prefetch" href="/assets/1.html-5jC9ix_K.js" as="script"><link rel="prefetch" href="/assets/1.html-NiCWhegR.js" as="script"><link rel="prefetch" href="/assets/1.html-BQb_IDQh.js" as="script"><link rel="prefetch" href="/assets/1.html-BzpLhtGa.js" as="script"><link rel="prefetch" href="/assets/2.html-NMvPjBQw.js" as="script"><link rel="prefetch" href="/assets/3.html-C5aq0F2c.js" as="script"><link rel="prefetch" href="/assets/index.html-4sgsU5Lu.js" as="script"><link rel="prefetch" href="/assets/1.html-DOhOW3gm.js" as="script"><link rel="prefetch" href="/assets/2.html-C6zSQp25.js" as="script"><link rel="prefetch" href="/assets/3.html-DxDhcgmN.js" as="script"><link rel="prefetch" href="/assets/1.html-DI8DKUuG.js" as="script"><link rel="prefetch" href="/assets/2.html-BzMgA0As.js" as="script"><link rel="prefetch" href="/assets/3.html-k7SZh6lZ.js" as="script"><link rel="prefetch" href="/assets/20240523.html-D2V4DHdd.js" as="script"><link rel="prefetch" href="/assets/20240524.html-DMy57Jul.js" as="script"><link rel="prefetch" href="/assets/20240527.html-B7JJbRou.js" as="script"><link rel="prefetch" href="/assets/20240612.html-D2Lmf3x-.js" as="script"><link rel="prefetch" href="/assets/20240818.html-CmpuXxZU.js" as="script"><link rel="prefetch" href="/assets/20240830.html-B3wgdxnA.js" as="script"><link rel="prefetch" href="/assets/20240905.html-B5XBQ8Vc.js" as="script"><link rel="prefetch" href="/assets/20240911.html-D5jdvVRs.js" as="script"><link rel="prefetch" href="/assets/20240912.html-RiB5FOpZ.js" as="script"><link rel="prefetch" href="/assets/20240925.html-DqAx8qq4.js" as="script"><link rel="prefetch" href="/assets/20241128.html-DmBDY-JZ.js" as="script"><link rel="prefetch" href="/assets/guide.html-FYjqZxxc.js" as="script"><link rel="prefetch" href="/assets/redis.html-Cw_34ZiA.js" as="script"><link rel="prefetch" href="/assets/api.html-C_nCCaiO.js" as="script"><link rel="prefetch" href="/assets/home.html-DJiNjL9m.js" as="script"><link rel="prefetch" href="/assets/plugin.html-KOHOYnEB.js" as="script"><link rel="prefetch" href="/assets/theme.html-2Akivgg2.js" as="script"><link rel="prefetch" href="/assets/404.html-0RlklDAJ.js" as="script"><link rel="prefetch" href="/assets/Valine.min-Dm4Ijz6H.js" as="script"><link rel="prefetch" href="/assets/giscus-aTimukGI-CKTvSCx2.js" as="script"><link rel="prefetch" href="/assets/setupDevtools-7MC2TMWH-BWdSLITi.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><div class="theme-container series--no show-catalog"><header class="navbar-container not-open"><div class="navbar-inner"><div class="site-brand nav-item"><img class="logo" src="/p.jpeg" alt="AQIU的日志"><a href="/" class="site-name can-hide">AQIU的日志</a></div><div class="nav-item navbar-links-wrapper" style=""><div><form class="search-box" role="search"><input type="search" autocomplete="off" spellcheck="false" value><!----></form></div><nav class="navbar-links"><!--[--><div class="navbar-links__item"><a href="/" class="link" aria-label="Home"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->Home<!--]--></span></span><!--[--><!--]--></a></div><div class="navbar-links__item"><div class="dropdown-link"><button class="dropdown-link__title" type="button" aria-label="Docs"><span class="xicon-container left title"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->Docs<!--]--></span></span><span class="arrow down"></span></button><button class="dropdown-link--mobile__title" type="button" aria-label="Docs"><span class="title"><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->Docs<!--]--></span></span></span><span class="right arrow"></span></button><ul style="display:none;" class="dropdown-link__container"><!--[--><li class="dropdown-link__item"><a href="/docs/theme-reco/theme" class="link" aria-label="区块链"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->区块链<!--]--></span></span><!--[--><!--]--></a></li><li class="dropdown-link__item"><a href="/blogs/other/guide" class="link" aria-label="版权保护"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->版权保护<!--]--></span></span><!--[--><!--]--></a></li><!--]--></ul></div></div><!--]--><!----><span class="xicon-container btn-toggle-dark-mode btn--dark-mode navbar-links__item"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewbox="0 0 32 32" style="width:20px;height:20px;font-size:20px;color:;"><path d="M15 2h2v3h-2z" fill="currentColor"></path><path d="M27 15h3v2h-3z" fill="currentColor"></path><path d="M15 27h2v3h-2z" fill="currentColor"></path><path d="M2 15h3v2H2z" fill="currentColor"></path><path d="M5.45 6.884l1.414-1.415l2.121 2.122l-1.414 1.414z" fill="currentColor"></path><path d="M23 7.58l2.121-2.12l1.414 1.414l-2.121 2.121z" fill="currentColor"></path><path d="M23.002 24.416l1.415-1.414l2.12 2.122l-1.413 1.414z" fill="currentColor"></path><path d="M5.47 25.13L7.59 23L9 24.42l-2.12 2.12l-1.41-1.41z" fill="currentColor"></path><path d="M16 8a8 8 0 1 0 8 8a8 8 0 0 0-8-8zm0 14a6 6 0 0 1 0-12z" fill="currentColor"></path></svg></span><ul class="social-links navbar-links__item"><!--[--><!--]--></ul></nav><span class="xicon-container btn-toggle-menus"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewbox="0 0 32 32" style="width:20px;height:20px;font-size:20px;color:;"><circle cx="16" cy="8" r="2" fill="currentColor"></circle><circle cx="16" cy="16" r="2" fill="currentColor"></circle><circle cx="16" cy="24" r="2" fill="currentColor"></circle></svg></span></div></div></header><!----><!----><!----><div class="theme-main" style=""><!----><!--[--><main class="page-container"><div class="page-content"><h1 class="page-title">CUDA 分享</h1><div class="page-info"><span class="xicon-container left"><!--[--><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewbox="0 0 32 32" class="xicon-icon" style="width:18px;height:18px;font-size:18px;color:;"><path d="M16 4a5 5 0 1 1-5 5a5 5 0 0 1 5-5m0-2a7 7 0 1 0 7 7a7 7 0 0 0-7-7z" fill="currentColor"></path><path d="M26 30h-2v-5a5 5 0 0 0-5-5h-6a5 5 0 0 0-5 5v5H6v-5a7 7 0 0 1 7-7h6a7 7 0 0 1 7 7z" fill="currentColor"></path></svg><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->Aqiu<!--]--></span></span><span class="xicon-container left"><!--[--><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewbox="0 0 32 32" class="xicon-icon" style="width:18px;height:18px;font-size:18px;color:;"><path d="M26 4h-4V2h-2v2h-8V2h-2v2H6c-1.1 0-2 .9-2 2v20c0 1.1.9 2 2 2h20c1.1 0 2-.9 2-2V6c0-1.1-.9-2-2-2zm0 22H6V12h20v14zm0-16H6V6h4v2h2V6h8v2h2V6h4v4z" fill="currentColor"></path></svg><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->2022/05/30<!--]--></span></span><span class="xicon-container left"><!--[--><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewbox="0 0 32 32" class="xicon-icon" style="width:18px;height:18px;font-size:18px;color:;"><path d="M11.17 6l3.42 3.41l.58.59H28v16H4V6h7.17m0-2H4a2 2 0 0 0-2 2v20a2 2 0 0 0 2 2h24a2 2 0 0 0 2-2V10a2 2 0 0 0-2-2H16l-3.41-3.41A2 2 0 0 0 11.17 4z" fill="currentColor"></path></svg><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[--><!--[--><a href="/categories/cuda/1.html" class="">cuda</a><!--]--><!--]--></span></span><span class="xicon-container left"><!--[--><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewbox="0 0 32 32" class="xicon-icon" style="width:18px;height:18px;font-size:18px;color:;"><path d="M10 14a4 4 0 1 1 4-4a4.005 4.005 0 0 1-4 4zm0-6a2 2 0 1 0 1.998 2.004A2.002 2.002 0 0 0 10 8z" fill="currentColor"></path><path d="M16.644 29.415L2.586 15.354A2 2 0 0 1 2 13.941V4a2 2 0 0 1 2-2h9.941a2 2 0 0 1 1.414.586l14.06 14.058a2 2 0 0 1 0 2.828l-9.943 9.943a2 2 0 0 1-2.829 0zM4 4v9.942L18.058 28L28 18.058L13.942 4z" fill="currentColor"></path></svg><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[--><!--[--><a href="/tags/jiqixuexi/1.html" class="">机器学习</a><!--]--><!--]--></span></span><!----></div><div class="theme-reco-md-content"><div><h1 id="_0-cuda在四层架构里的位置" tabindex="-1"><a class="header-anchor" href="#_0-cuda在四层架构里的位置"><span>0 CUDA在四层架构里的位置</span></a></h1><p>生成式AI和大模型的智能涌现，带来了全新的计算范式，不可避免地改变各行各业。这也使得整个IT的技术栈从原来的三层，变成包括芯片、框架、模型、应用在内的四层结构，新模型层一出现，即成为人工智能发展的热点。其中在框架层、模型层、应用层三层百度专利储备国内第一。</p><p>百度是全球为数不多，进行全栈布局人工智能的公司。从高端芯片昆仑芯，到飞桨深度学习框架，再到文心预训练大模型，再到自动驾驶、小度、智能云等应用，都有领先业界的自研技术。</p><p><img src="https://rte.weiyun.baidu.com/wiki/attach/image/api/imageDownloadAddress?attachId=8d35e32f6cc84c1ca61a7ab83ce8cd3a&amp;docGuid=nLYcJuLy91oo99" alt="img"></p><ul><li>芯片层（NVIDIA、昆仑芯）</li></ul><p><img src="https://rte.weiyun.baidu.com/wiki/attach/image/api/imageDownloadAddress?attachId=48deda1c2dd9425c8754529a5c57a20c&amp;docGuid=nLYcJuLy91oo99" alt="img"></p><p><a href="https://ku.baidu-int.com/knowledge/HFVrC7hq1Q/pKzJfZczuc/9m_zE2aj2i/nLYcJuLy91oo99?block_id=docyg-c0b891d0-05f2-11ee-9663-9df2a06c4427" target="_blank" rel="noopener noreferrer">昆仑芯<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>（北京）科技有限公司前身为百度智能芯片及架构部，于2021年4月完成独立融资，首轮估值约130亿元。在国内最早布局AI加速领域，深耕10余年，是一家在体系结构、芯片实现、软件系统和场景应用均有深厚积累的AI芯片企业。 昆仑芯科技已实现两代通用AI芯片产品的量产及落地应用，两代产品先后斩获2020、2021年中国芯“优秀技术创新产品”奖。昆仑芯1代AI芯片于2020年量产，在百度搜索引擎、小度等业务中部署数万片，是国内唯一一款经历过互联网大规模核心算法考验的云端AI芯片，同时也被广泛部署在互联网、工业制造、智慧金融、智慧交通等领域。 搭载新一代架构XPU-R的昆仑芯2代AI芯片于2021年6月回片并当天点亮，8月量产发布。昆仑芯2代AI芯片是国内首款采用GDDR6显存的通用AI芯片，相比昆仑芯1代AI芯片性能提升2-3倍，且在通用性、易用性方面也有显著增强。</p><p><img src="https://rte.weiyun.baidu.com/wiki/attach/image/api/imageDownloadAddress?attachId=6d774ec9e0f34637af50770ba433b351&amp;docGuid=nLYcJuLy91oo99" alt="img"></p><p>英伟达是一家人工智能计算公司，成立于1993年，总部位于美国加利福尼亚州圣克拉拉市。公司的主要业务是设计、制造和销售计算机芯片，包括GPU、Tegra、Qudro和DGX。</p><p>英伟达的GPU在深度学习领域有着广泛的应用，能够帮助计算机更好地处理图像、语音、文本等数据。通过不断更新GPU架构和算法技术，英伟达推动了人工智能的发展。</p><p>除此之外，英伟达还通过合作和投资来加速人工智能的发展和应用。例如，2016年，英伟达和谷歌合作开发了深度学习框架TensorFlow，并在2017年宣布将向全球100所大学捐赠英伟达DGX-1人工智能超级计算机。此外，英伟达还与特斯拉、奔驰、本田等公司合作，将人工智能技术应用于自动驾驶等领域。</p><ul><li>框架层</li></ul><p><a href="https://www.tensorflow.org/?hl=zh-cn" target="_blank" rel="noopener noreferrer"><strong>TensorFlow</strong><span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p>TensorFlow由Google智能机器研究部门研发；TensorFlow编程接口支持Python和C++。随着1.0版本的公布，相继支持了Java、Go、R和Haskell API的alpha版本。2.0版本又把Keras的相关API都嵌入到tf中，使得其功能更加强大。但由于版本变动过大，因此1.0版本的代码在2.0版本好多都报错，造成版本升级迭代困难。</p><p>在2017年，Tensorflow独占鳌头，处于深度学习框架的领先地位；但截至目前已经和Pytorch不争上下，甚至略输入Pytorch。</p><p><strong>Tensorflow目前主要在工业级领域处于领先地位</strong>。</p><p><a href="https://pytorch.org/" target="_blank" rel="noopener noreferrer">PyTorch<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p>Pytorch目前是由Facebook人工智能学院提供支持服务的。</p><p><strong>Pytorch目前主要在学术研究方向领域处于领先地位，许多学术论文都是用pytorch编写的，因此使用范围更广。</strong></p><p>其优点在于：PyTorch可以使用强大的GPU加速的Tensor计算（比如：Numpy的使用）以及可以构建带有autograd的深度神经网络。</p><p>同时，PyTorch 的代码很简洁、易于使用、支持计算过程中的动态图而且内存使用很高效，版本之间差异也不大，没有升级方面的困难。</p><p><a href="https://www.paddlepaddle.org.cn/" target="_blank" rel="noopener noreferrer"><strong>PaddlePaddle</strong><span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><p>飞桨（PaddlePaddle）以百度多年的深度学习技术研究和业务应用为基础，集深度学习核心训练和推理框架、基础模型库、端到端开发套件、丰富的工具组件于一体，是中国首个自主研发、功能完备、开源开放的产业级深度学习平台。国内的可以优先选择paddlepaddle，这是因为百度在大力推广该框架，能提供丰富的算力支持和技术支持，且百度开源了众多模型和应用，是人工智能入门的一个好选择，且能在模型上面进行一系列魔改。</p><p>https://github.com/PaddlePaddle</p><p><img src="https://rte.weiyun.baidu.com/wiki/attach/image/api/imageDownloadAddress?attachId=34ef5e89ff17490c85a05b188d2aaacd&amp;docGuid=nLYcJuLy91oo99" alt="img"></p><ul><li>模型层</li></ul><p>ChatGPT</p><p>文心一言</p><ul><li>应用层</li></ul><p>Midjourney：https://www.midjourney.com/home/?callbackUrl=%2Fapp%2F</p><p>文心一格</p><p><a href="https://aistudio.baidu.com/aistudio/projectdetail/4462918" target="_blank" rel="noopener noreferrer">国内外主流AI绘画教程案例合集 - 飞桨AI Studio星河社区<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a></p><h1 id="_1-黄仁勋和nvidia" tabindex="-1"><a class="header-anchor" href="#_1-黄仁勋和nvidia"><span>1 黄仁勋和NVIDIA</span></a></h1><p>黄仁勋是硅谷最有影响力的华人。</p><p><img src="https://rte.weiyun.baidu.com/wiki/attach/image/api/imageDownloadAddress?attachId=2f30ecc71e544377a803c295933bb265&amp;docGuid=nLYcJuLy91oo99" alt="img"></p><p>1993年，他跟两个朋友一起创办了芯片设计公司 Nvidia（中文名&quot;英伟达&quot;）。当时是小公司，可现在是美国第五大科技公司（仅次于苹果、微软、谷歌和亚马逊）。</p><p>10年里，NVIDIA股票涨了105倍（https://www.chinaz.com/2023/0527/1528509.shtml）。</p><p><img src="https://rte.weiyun.baidu.com/wiki/attach/image/api/imageDownloadAddress?attachId=4a1f65fec38e4e63b9261c521f391304&amp;docGuid=nLYcJuLy91oo99" alt="img"></p><p>黄仁勋台大毕业典礼演讲（中文）：https://redian.news/wxnews/412537</p><p>摘要：</p><p>黄仁勋说，<strong>为什么要创立 Nvidia？因为我们看好加速计算。</strong></p><p>人类对计算速度的要求一定会越来越高，CPU 只能做通用计算，加速计算需要定制的专用硬件，所以我们的创业目标就是加速计算的硬件。</p><p>市场需求量最大的加速计算硬件，就是游戏的图形芯片（GPU），所以我们选择游戏显卡作为创业产品。我们只做芯片设计，生产全部外包。</p><p>1994年，我们的第一个客户是日本游戏公司 SEGA，我们为它的游戏主机设计显卡。</p><p><img src="https://rte.weiyun.baidu.com/wiki/attach/image/api/imageDownloadAddress?attachId=338dd30ed9f643c99ed2432d0e2903ac&amp;docGuid=nLYcJuLy91oo99" alt="img"></p><p>（图片说明：Nvidia 的第一个产品，SEGA 游戏机的 <a href="https://segaretro.org/NV1" target="_blank" rel="noopener noreferrer">NV1 显卡<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>。）</p><p>但是第二年，微软发布了 Windows 平台的图形接口 Direct3D。我们一下子就慌了，因为它跟我们的设计是冲突的。</p><p>我们最终选择中止 SEGA 的合约，<strong>改为 Windows 平台开发 GPU</strong>。这是一步险棋，因为 SEGA 是我们唯一的客户，却被我们踢走了。我们的资金只能支持6个月，如果这点时间里面，拿不出新产品，我们就只有倒闭了。</p><p>幸运的是，快要没钱的时候，我们设计出了 Riva 128，这块芯片取得了成功。到了1997年底，它的出货量超过100万张，我们就这样活了下来。</p><p><img src="https://rte.weiyun.baidu.com/wiki/attach/image/api/imageDownloadAddress?attachId=29ea4a645b654792b1fdbd2b116d3a37&amp;docGuid=nLYcJuLy91oo99" alt="img"></p><p>我们为 Windows 平台设计显卡，一直干了10年。</p><p>虽然产品很受欢迎，但是有一个问题：<strong>人们只用这些显卡打游戏，无法用于其他的加速计算。</strong> 因为那时的 GPU 必须通过 Windows 的接口使用，受制于操作系统，用户无法直接操作 GPU，很难将其用于自己的用途。</p><p>为了扩展 GPU 的用途，<strong>2007年我们推出了 CUDA 框架，让用户可以操作 GPU 底层接口</strong>，定制化编程，满足自己的加速计算需求。GPU 从此可以用于科学运算、物理模拟等各方面。</p><p><img src="https://rte.weiyun.baidu.com/wiki/attach/image/api/imageDownloadAddress?attachId=6a695875af164fd38c59d00bd393eaf9&amp;docGuid=nLYcJuLy91oo99" alt="img"></p><p>令人失望的是，市场需求始终不旺，而我们推进 CUDA 的成本非常高。那几年，我们的利润受到严重拖累，股价低迷。内部也出现分歧，有人提出放弃 CUDA。</p><p>谁能想到，命运的转折点突然出现了。<strong>2014年，人们发现 CUDA 能够满足 AI 训练的大量计算</strong>，它一下子就变得异常火爆。随着 AI 的快速发展，我们从此走上了康庄大道，股价一飞冲天。</p><p>除了 AI，我们也尝试把加速计算推广到其他新兴领域。</p><p>2007年 iPhone 诞生了，手机芯片成为了一个超级市场。我们开始考虑为安卓手机开发芯片。</p><p>但是，手机芯片是集成的，CPU、GPU、通信芯片（调制解调器）做在一起。<strong>如果我们要做安卓芯片，就必须研发通信芯片。这跟我们的加速计算方向是不符合的。</strong></p><p>我们不得不做出一个艰难的决定：放弃手机市场。为了弥补这个损失，<strong>我们选择进军另一个更符合我们的市场：自动驾驶的车用芯片。</strong> 自动驾驶的计算量非常大，市场也很广阔。</p><h1 id="_2-cuda-一个在gpu-上计算的新架构-compute-unified-device-architecture" tabindex="-1"><a class="header-anchor" href="#_2-cuda-一个在gpu-上计算的新架构-compute-unified-device-architecture"><span>2 CUDA：一个在GPU 上计算的新架构（Compute Unified Device Architecture）</span></a></h1><p>https://www.nvidia.com/en-us/training/online/</p><p>2006年，NVIDIA公司发布了<a href="https://link.zhihu.com/?target=http%3A//docs.nvidia.com/cuda/" target="_blank" rel="noopener noreferrer">CUDA<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>，CUDA是建立在NVIDIA的CPUs上的一个通用并行计算平台和编程模型，基于CUDA编程可以利用GPUs的并行计算引擎来更加高效地解决比较复杂的计算难题。近年来，GPU最成功的一个应用就是深度学习领域，基于GPU的并行计算已经成为训练深度学习模型的标配。目前，最新的CUDA版本为CUDA 9。</p><p>GPU并不是一个独立运行的计算平台，而需要与CPU协同工作，可以看成是CPU的协处理器，因此当我们在说GPU并行计算时，其实是指的基于CPU+GPU的异构计算架构。在异构计算架构中，GPU与CPU通过PCIe总线连接在一起来协同工作，CPU所在位置称为为主机端（host），而GPU所在位置称为设备端（device），如下图所示。</p><p><img src="https://rte.weiyun.baidu.com/wiki/attach/image/api/imageDownloadAddress?attachId=d6a3cebffea84e54b2172b81b60ae838&amp;docGuid=nLYcJuLy91oo99" alt="img"></p><p>基于CPU+GPU的异构计算. 来源：Preofessional CUDA® C Programming</p><p>可以看到GPU包括更多的运算核心，其特别适合数据并行的计算密集型任务，如大型矩阵运算，而CPU的运算核心较少，但是其可以实现复杂的逻辑运算，因此其适合控制密集型任务。另外，CPU上的线程是重量级的，上下文切换开销大，但是GPU由于存在很多核心，其线程是轻量级的。因此，基于CPU+GPU的异构计算平台可以优势互补，CPU负责处理逻辑复杂的串行程序，而GPU重点处理数据密集型的并行计算程序，从而发挥最大功效。</p><p><img src="https://rte.weiyun.baidu.com/wiki/attach/image/api/imageDownloadAddress?attachId=3c106796e9f345749366c270da25dc9a&amp;docGuid=nLYcJuLy91oo99" alt="img"></p><p>基于CPU+GPU的异构计算应用执行逻辑。</p><p>CUDA是NVIDIA公司所开发的GPU编程模型，它提供了GPU编程的简易接口，基于CUDA编程可以构建基于GPU计算的应用程序。CUDA提供了对其它编程语言的支持，如C/C++，Python，Fortran等语言。</p><p>典型的CUDA程序的执行流程如下：</p><ol><li>分配host内存，并进行数据初始化；</li><li>分配device内存，并从host将数据拷贝到device上；</li><li>调用CUDA的核函数在device上完成指定的运算；</li><li>将device上的运算结果拷贝到host上；</li><li>释放device和host上分配的内存。</li></ol><p>上面流程中最重要的一个过程是调用CUDA的核函数来执行并行计算，<a href="https://link.zhihu.com/?target=http%3A//docs.nvidia.com/cuda/cuda-c-programming-guide/index.html%23kernels" target="_blank" rel="noopener noreferrer">kernel<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a>是CUDA中一个重要的概念，kernel是在device上线程中并行执行的函数，核函数用<code>__global__</code>符号声明，在调用时需要用<code>&lt;&lt;&lt;grid, block&gt;&gt;&gt;</code>来指定kernel要执行的线程数量，在CUDA中，每一个线程都要执行核函数，并且每个线程会分配一个唯一的线程号thread ID，这个ID值可以通过核函数的内置变量<code>threadIdx</code>来获得。</p><p>由于GPU实际上是异构模型，所以需要区分host和device上的代码，在CUDA中是通过函数类型限定词开区别host和device上的函数，主要的三个函数类型限定词如下：</p><ul><li><code>__global__</code>：在device上执行，从host中调用（一些特定的GPU也可以从device上调用），返回类型必须是<code>void</code>，不支持可变参数参数，不能成为类成员函数。注意用<code>__global__</code>定义的kernel是异步的，这意味着host不会等待kernel执行完就执行下一步。</li><li><code>__device__</code>：在device上执行，单仅可以从device中调用，不可以和<code>__global__</code>同时用。</li><li><code>__host__</code>：在host上执行，仅可以从host上调用，一般省略不写，不可以和<code>__global__</code>同时用，但可和<code>__device__</code>，此时函数会在device和host都编译。</li></ul><p>要深刻理解kernel，必须要对kernel的线程层次结构有一个清晰的认识。首先GPU上很多并行化的轻量级线程。kernel在device上执行时实际上是启动很多线程，一个kernel所启动的所有线程称为一个<strong>网格</strong>（grid），同一个网格上的线程共享相同的全局内存空间，grid是线程结构的第一层次，而网格又可以分为很多<strong>线程块</strong>（block），一个线程块里面包含很多线程，这是第二个层次。线程两层组织结构如下图所示，这是一个gird和block均为2-dim的线程组织。grid和block都是定义为<code>dim3</code>类型的变量，<code>dim3</code>可以看成是包含三个无符号整数（x，y，z）成员的结构体变量，在定义时，缺省值初始化为1。因此grid和block可以灵活地定义为1-dim，2-dim以及3-dim结构，对于图中结构（主要水平方向为x轴），定义的grid和block如下所示，kernel在调用时也必须通过<a href="https://link.zhihu.com/?target=http%3A//docs.nvidia.com/cuda/cuda-c-programming-guide/index.html%23execution-configuration" target="_blank" rel="noopener noreferrer">执行配置<span><svg class="external-link-icon" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path><polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg><span class="external-link-icon-sr-only">open in new window</span></span></a><code>&lt;&lt;&lt;grid, block&gt;&gt;&gt;</code>来指定kernel所使用的线程数及结构。</p><p><img src="https://rte.weiyun.baidu.com/wiki/attach/image/api/imageDownloadAddress?attachId=7b1616be276b4324949b3495643512ad&amp;docGuid=nLYcJuLy91oo99" alt="img"></p><h2 id="加速单个线程块for循环" tabindex="-1"><a class="header-anchor" href="#加速单个线程块for循环"><span>加速单个线程块for循环</span></a></h2><div class="language-c line-numbers-mode" data-highlighter="prismjs" data-ext="c" data-title="c"><pre><code><span class="line"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;stdio.h&gt;</span></span></span>
<span class="line"></span>
<span class="line">__global__ <span class="token keyword">void</span> <span class="token function">loop</span><span class="token punctuation">(</span><span class="token keyword">int</span> N<span class="token punctuation">)</span></span>
<span class="line"><span class="token punctuation">{</span></span>
<span class="line">  <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> N<span class="token punctuation">;</span> <span class="token operator">++</span>i<span class="token punctuation">)</span></span>
<span class="line">  <span class="token punctuation">{</span></span>
<span class="line">    <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">&quot;This is iteration number %d\n&quot;</span><span class="token punctuation">,</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">)</span><span class="token punctuation">;</span></span>
<span class="line">  <span class="token punctuation">}</span></span>
<span class="line"><span class="token punctuation">}</span></span>
<span class="line"></span>
<span class="line"><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span></span>
<span class="line"><span class="token punctuation">{</span></span>
<span class="line">  <span class="token keyword">int</span> N <span class="token operator">=</span> <span class="token number">10</span><span class="token punctuation">;</span></span>
<span class="line">  loop<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span>N<span class="token punctuation">)</span><span class="token punctuation">;</span></span>
<span class="line">  <span class="token function">cudaDeviceSynchronize</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span></span>
<span class="line"><span class="token punctuation">}</span></span>
<span class="line"><span class="token function">cudaDeviceSynchronize</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>与许多 C/C++ 代码不同，核函数启动方式为异步：CPU 代码将继续执行而无需等待核函数完成启动。</li><li>调用 CUDA 运行时提供的函数cudaDeviceSynchronize将导致主机 (CPU) 代码暂作等待，直至设备 (GPU) 代码执行完成，才能在 CPU 上恢复执行</li></ul><h2 id="加速多个线程块的for循环" tabindex="-1"><a class="header-anchor" href="#加速多个线程块的for循环"><span>加速多个线程块的for循环</span></a></h2><div class="language-c line-numbers-mode" data-highlighter="prismjs" data-ext="c" data-title="c"><pre><code><span class="line"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;stdio.h&gt;</span></span></span>
<span class="line"></span>
<span class="line">__global__ <span class="token keyword">void</span> <span class="token function">loop</span><span class="token punctuation">(</span><span class="token keyword">int</span> N<span class="token punctuation">)</span></span>
<span class="line"><span class="token punctuation">{</span></span>
<span class="line">  </span>
<span class="line">  <span class="token keyword">int</span> j <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span></span>
<span class="line">  <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">&quot;%d&quot;</span><span class="token punctuation">,</span> j<span class="token punctuation">)</span><span class="token punctuation">;</span></span>
<span class="line"><span class="token punctuation">}</span></span>
<span class="line"></span>
<span class="line"><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span></span>
<span class="line"><span class="token punctuation">{</span></span>
<span class="line">  <span class="token keyword">int</span> N <span class="token operator">=</span> <span class="token number">10</span><span class="token punctuation">;</span></span>
<span class="line">  loop<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span>N<span class="token punctuation">)</span><span class="token punctuation">;</span></span>
<span class="line">  <span class="token function">cudaDeviceSynchronize</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span></span>
<span class="line"><span class="token punctuation">}</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="加速2d矩阵乘法应用" tabindex="-1"><a class="header-anchor" href="#加速2d矩阵乘法应用"><span>加速2D矩阵乘法应用</span></a></h2><div class="language-cpp line-numbers-mode" data-highlighter="prismjs" data-ext="cpp" data-title="cpp"><pre><code><span class="line"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">include</span> <span class="token string">&lt;stdio.h&gt;</span></span></span>
<span class="line"></span>
<span class="line"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name">N</span>  <span class="token expression"><span class="token number">64</span></span></span></span>
<span class="line"></span>
<span class="line"><span class="token comment">/*</span>
<span class="line"> * GPU 核函数</span>
<span class="line"> */</span></span>
<span class="line">__global__ <span class="token keyword">void</span> <span class="token function">matrixMulGPU</span><span class="token punctuation">(</span> <span class="token keyword">int</span> <span class="token operator">*</span> a<span class="token punctuation">,</span> <span class="token keyword">int</span> <span class="token operator">*</span> b<span class="token punctuation">,</span> <span class="token keyword">int</span> <span class="token operator">*</span> c <span class="token punctuation">)</span></span>
<span class="line"><span class="token punctuation">{</span></span>
<span class="line">  <span class="token keyword">int</span> val <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span></span>
<span class="line"></span>
<span class="line">  <span class="token keyword">int</span> row <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span></span>
<span class="line">  <span class="token keyword">int</span> col <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>y <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>y <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>y<span class="token punctuation">;</span></span>
<span class="line"></span>
<span class="line">  <span class="token keyword">if</span> <span class="token punctuation">(</span>row <span class="token operator">&lt;</span> N <span class="token operator">&amp;&amp;</span> col <span class="token operator">&lt;</span> N<span class="token punctuation">)</span></span>
<span class="line">  <span class="token punctuation">{</span></span>
<span class="line">    <span class="token keyword">for</span> <span class="token punctuation">(</span> <span class="token keyword">int</span> k <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> k <span class="token operator">&lt;</span> N<span class="token punctuation">;</span> <span class="token operator">++</span>k <span class="token punctuation">)</span></span>
<span class="line">      val <span class="token operator">+=</span> a<span class="token punctuation">[</span>row <span class="token operator">*</span> N <span class="token operator">+</span> k<span class="token punctuation">]</span> <span class="token operator">*</span> b<span class="token punctuation">[</span>k <span class="token operator">*</span> N <span class="token operator">+</span> col<span class="token punctuation">]</span><span class="token punctuation">;</span></span>
<span class="line">    c<span class="token punctuation">[</span>row <span class="token operator">*</span> N <span class="token operator">+</span> col<span class="token punctuation">]</span> <span class="token operator">=</span> val<span class="token punctuation">;</span></span>
<span class="line">  <span class="token punctuation">}</span></span>
<span class="line"><span class="token punctuation">}</span></span>
<span class="line"></span>
<span class="line"><span class="token comment">/*</span>
<span class="line"> * CPU函数，验证GPU核函数的运算结果</span>
<span class="line"> */</span></span>
<span class="line"></span>
<span class="line"><span class="token keyword">void</span> <span class="token function">matrixMulCPU</span><span class="token punctuation">(</span> <span class="token keyword">int</span> <span class="token operator">*</span> a<span class="token punctuation">,</span> <span class="token keyword">int</span> <span class="token operator">*</span> b<span class="token punctuation">,</span> <span class="token keyword">int</span> <span class="token operator">*</span> c <span class="token punctuation">)</span></span>
<span class="line"><span class="token punctuation">{</span></span>
<span class="line">  <span class="token keyword">int</span> val <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span></span>
<span class="line"></span>
<span class="line">  <span class="token keyword">for</span><span class="token punctuation">(</span> <span class="token keyword">int</span> row <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> row <span class="token operator">&lt;</span> N<span class="token punctuation">;</span> <span class="token operator">++</span>row <span class="token punctuation">)</span></span>
<span class="line">    <span class="token keyword">for</span><span class="token punctuation">(</span> <span class="token keyword">int</span> col <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> col <span class="token operator">&lt;</span> N<span class="token punctuation">;</span> <span class="token operator">++</span>col <span class="token punctuation">)</span></span>
<span class="line">    <span class="token punctuation">{</span></span>
<span class="line">      val <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span></span>
<span class="line">      <span class="token keyword">for</span> <span class="token punctuation">(</span> <span class="token keyword">int</span> k <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> k <span class="token operator">&lt;</span> N<span class="token punctuation">;</span> <span class="token operator">++</span>k <span class="token punctuation">)</span></span>
<span class="line">        val <span class="token operator">+=</span> a<span class="token punctuation">[</span>row <span class="token operator">*</span> N <span class="token operator">+</span> k<span class="token punctuation">]</span> <span class="token operator">*</span> b<span class="token punctuation">[</span>k <span class="token operator">*</span> N <span class="token operator">+</span> col<span class="token punctuation">]</span><span class="token punctuation">;</span></span>
<span class="line">      c<span class="token punctuation">[</span>row <span class="token operator">*</span> N <span class="token operator">+</span> col<span class="token punctuation">]</span> <span class="token operator">=</span> val<span class="token punctuation">;</span></span>
<span class="line">    <span class="token punctuation">}</span></span>
<span class="line"><span class="token punctuation">}</span></span>
<span class="line"></span>
<span class="line"><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span></span>
<span class="line"><span class="token punctuation">{</span></span>
<span class="line">  <span class="token keyword">int</span> <span class="token operator">*</span>a<span class="token punctuation">,</span> <span class="token operator">*</span>b<span class="token punctuation">,</span> <span class="token operator">*</span>c_cpu<span class="token punctuation">,</span> <span class="token operator">*</span>c_gpu<span class="token punctuation">;</span> <span class="token comment">// 分配个地址空间用于存矩阵运算结果</span></span>
<span class="line"></span>
<span class="line">  <span class="token keyword">int</span> size <span class="token operator">=</span> N <span class="token operator">*</span> N <span class="token operator">*</span> <span class="token keyword">sizeof</span> <span class="token punctuation">(</span><span class="token keyword">int</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment">// N x N 矩阵</span></span>
<span class="line"></span>
<span class="line">  <span class="token comment">// 分配内存空间</span></span>
<span class="line">  <span class="token function">cudaMallocManaged</span> <span class="token punctuation">(</span><span class="token operator">&amp;</span>a<span class="token punctuation">,</span> size<span class="token punctuation">)</span><span class="token punctuation">;</span></span>
<span class="line">  <span class="token function">cudaMallocManaged</span> <span class="token punctuation">(</span><span class="token operator">&amp;</span>b<span class="token punctuation">,</span> size<span class="token punctuation">)</span><span class="token punctuation">;</span></span>
<span class="line">  <span class="token function">cudaMallocManaged</span> <span class="token punctuation">(</span><span class="token operator">&amp;</span>c_cpu<span class="token punctuation">,</span> size<span class="token punctuation">)</span><span class="token punctuation">;</span></span>
<span class="line">  <span class="token function">cudaMallocManaged</span> <span class="token punctuation">(</span><span class="token operator">&amp;</span>c_gpu<span class="token punctuation">,</span> size<span class="token punctuation">)</span><span class="token punctuation">;</span></span>
<span class="line"></span>
<span class="line">  <span class="token comment">// 初始化; 创建个2维空间</span></span>
<span class="line">  <span class="token keyword">for</span><span class="token punctuation">(</span> <span class="token keyword">int</span> row <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> row <span class="token operator">&lt;</span> N<span class="token punctuation">;</span> <span class="token operator">++</span>row <span class="token punctuation">)</span></span>
<span class="line">    <span class="token keyword">for</span><span class="token punctuation">(</span> <span class="token keyword">int</span> col <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> col <span class="token operator">&lt;</span> N<span class="token punctuation">;</span> <span class="token operator">++</span>col <span class="token punctuation">)</span></span>
<span class="line">    <span class="token punctuation">{</span></span>
<span class="line">      a<span class="token punctuation">[</span>row<span class="token operator">*</span>N <span class="token operator">+</span> col<span class="token punctuation">]</span> <span class="token operator">=</span> row<span class="token punctuation">;</span></span>
<span class="line">      b<span class="token punctuation">[</span>row<span class="token operator">*</span>N <span class="token operator">+</span> col<span class="token punctuation">]</span> <span class="token operator">=</span> col<span class="token operator">+</span><span class="token number">2</span><span class="token punctuation">;</span></span>
<span class="line">      c_cpu<span class="token punctuation">[</span>row<span class="token operator">*</span>N <span class="token operator">+</span> col<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span></span>
<span class="line">      c_gpu<span class="token punctuation">[</span>row<span class="token operator">*</span>N <span class="token operator">+</span> col<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span></span>
<span class="line">    <span class="token punctuation">}</span></span>
<span class="line"></span>
<span class="line">  <span class="token comment">/*</span>
<span class="line">   * 声明 `threads_per_block`  `number_of_blocks` </span>
<span class="line">   */</span></span>
<span class="line"></span>
<span class="line">  dim3 <span class="token function">threads_per_block</span> <span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span></span>
<span class="line">  dim3 <span class="token function">number_of_blocks</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>N <span class="token operator">/</span> threads_per_block<span class="token punctuation">.</span>x<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>N <span class="token operator">/</span> threads_per_block<span class="token punctuation">.</span>y<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span></span>
<span class="line"></span>
<span class="line">  matrixMulGPU <span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span> number_of_blocks<span class="token punctuation">,</span> threads_per_block <span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> <span class="token punctuation">(</span> a<span class="token punctuation">,</span> b<span class="token punctuation">,</span> c_gpu <span class="token punctuation">)</span><span class="token punctuation">;</span></span>
<span class="line"></span>
<span class="line">  <span class="token function">cudaDeviceSynchronize</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span></span>
<span class="line"></span>
<span class="line">  <span class="token comment">// 调用CPU函数</span></span>
<span class="line">  <span class="token function">matrixMulCPU</span><span class="token punctuation">(</span> a<span class="token punctuation">,</span> b<span class="token punctuation">,</span> c_cpu <span class="token punctuation">)</span><span class="token punctuation">;</span></span>
<span class="line"></span>
<span class="line">  <span class="token comment">// 比较两个结果是否相等</span></span>
<span class="line">  <span class="token keyword">bool</span> error <span class="token operator">=</span> <span class="token boolean">false</span><span class="token punctuation">;</span></span>
<span class="line">  <span class="token keyword">for</span><span class="token punctuation">(</span> <span class="token keyword">int</span> row <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> row <span class="token operator">&lt;</span> N <span class="token operator">&amp;&amp;</span> <span class="token operator">!</span>error<span class="token punctuation">;</span> <span class="token operator">++</span>row <span class="token punctuation">)</span></span>
<span class="line">    <span class="token keyword">for</span><span class="token punctuation">(</span> <span class="token keyword">int</span> col <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">;</span> col <span class="token operator">&lt;</span> N <span class="token operator">&amp;&amp;</span> <span class="token operator">!</span>error<span class="token punctuation">;</span> <span class="token operator">++</span>col <span class="token punctuation">)</span></span>
<span class="line">      <span class="token keyword">if</span> <span class="token punctuation">(</span>c_cpu<span class="token punctuation">[</span>row <span class="token operator">*</span> N <span class="token operator">+</span> col<span class="token punctuation">]</span> <span class="token operator">!=</span> c_gpu<span class="token punctuation">[</span>row <span class="token operator">*</span> N <span class="token operator">+</span> col<span class="token punctuation">]</span><span class="token punctuation">)</span></span>
<span class="line">      <span class="token punctuation">{</span></span>
<span class="line">        <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">&quot;FOUND ERROR at c[%d][%d]\n&quot;</span><span class="token punctuation">,</span> row<span class="token punctuation">,</span> col<span class="token punctuation">)</span><span class="token punctuation">;</span></span>
<span class="line">        error <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span></span>
<span class="line">        <span class="token keyword">break</span><span class="token punctuation">;</span></span>
<span class="line">      <span class="token punctuation">}</span></span>
<span class="line">  <span class="token keyword">if</span> <span class="token punctuation">(</span><span class="token operator">!</span>error<span class="token punctuation">)</span></span>
<span class="line">    <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">&quot;Success!\n&quot;</span><span class="token punctuation">)</span><span class="token punctuation">;</span></span>
<span class="line"></span>
<span class="line">  <span class="token comment">// 释放内存</span></span>
<span class="line">  <span class="token function">cudaFree</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token function">cudaFree</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token punctuation">;</span></span>
<span class="line">  <span class="token function">cudaFree</span><span class="token punctuation">(</span> c_cpu <span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token function">cudaFree</span><span class="token punctuation">(</span> c_gpu <span class="token punctuation">)</span><span class="token punctuation">;</span></span>
<span class="line"><span class="token punctuation">}</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div></div></div><footer class="page-meta"><!----><div class="meta-item last-updated"><span class="xicon-container left meta-item-label"><!--[--><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewbox="0 0 32 32" class="xicon-icon" style="width:20px;height:20px;font-size:20px;color:;"><path d="M26 4h-4V2h-2v2h-8V2h-2v2H6c-1.1 0-2 .9-2 2v20c0 1.1.9 2 2 2h20c1.1 0 2-.9 2-2V6c0-1.1-.9-2-2-2zm0 22H6V12h20v14zm0-16H6V6h4v2h2V6h8v2h2V6h4v4z" fill="currentColor"></path></svg><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->Last Updated 2023/11/15 18:57:02<!--]--></span></span></div></footer><!----><!----></div><div class="page-catalog-container"><h5 class="tip">ON THIS PAGE</h5><ul><!--[--><!--[--><li class="page-catalog-menu-depth_2"><a aria-current="page" href="/blogs/cuda/4.html#加速单个线程块for循环" class="router-link-active router-link-exact-active link page-catalog-item page-catalog-item" aria-label="加速单个线程块for循环"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->加速单个线程块for循环<!--]--></span></span><!--[--><!--]--></a></li><!--]--><!--[--><li class="page-catalog-menu-depth_2"><a aria-current="page" href="/blogs/cuda/4.html#加速多个线程块的for循环" class="router-link-active router-link-exact-active link page-catalog-item page-catalog-item" aria-label="加速多个线程块的for循环"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->加速多个线程块的for循环<!--]--></span></span><!--[--><!--]--></a></li><!--]--><!--[--><li class="page-catalog-menu-depth_2"><a aria-current="page" href="/blogs/cuda/4.html#加速2d矩阵乘法应用" class="router-link-active router-link-exact-active link page-catalog-item page-catalog-item" aria-label="加速2D矩阵乘法应用"><!--[--><!--]--><span class="xicon-container left"><!--[--><!----><!--]--><span class="xicon-content" style="color:;font-size:14px;"><!--[-->加速2D矩阵乘法应用<!--]--></span></span><!--[--><!--]--></a></li><!--]--><!--]--></ul></div></main><!--]--></div></div><!--[--><!----><!----><!--]--><!--]--></div>
    <script type="module" src="/assets/app-vAERMEX6.js" defer></script>
  </body>
</html>
